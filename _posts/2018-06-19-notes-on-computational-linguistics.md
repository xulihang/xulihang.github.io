---
date: 2018-06-19 20:05:50+08:00
layout: post
title: 计算语言学概念整理
categories: 技术随笔
tags: CAT NLP
---

语言出身的我还是更喜欢用计算语言学这个词。本文整理一些相关概念。

## 齐夫定律（Zipf's law）

在一个大规模的语料库，统计词频，将词按词频从高到低排序。词的词频f和序号r相乘得到k，这个k近似于一个常数。

齐夫定律也叫省力法则，通俗的意义就是人们喜欢用少量的词表达想表达的意思。就比如诸葛亮，说话者和听者只听这一个词就能知道意思，如果再用什么孔明、卧龙，则比较费劲。

所以语料库的一个问题就是很多冷门的词出现的频率会很低。

## 停用词（stop words）

常用的词，如the，a，to等在处理中会被忽略。

## 词的归一化（Word Normalization）

在检索时经常用到。比如U.S.A和USA统一用USA存。也可以把一个词拓展，比如搜索window，同时出现windows和Windows的结果。

## 波特词干算法

适用于信息检索的词干还原方法，比如automate、automatic和automation都会被还原为automat，是一种比较粗暴的办法。

## 汉语离合词

汉语动词很多现象，使得计算机难以处理。比如游泳，会说成游了一会儿泳。

## 语言模型（language model）

用来统计某个句子或者一串词出现的概率或者某个词后出现另一个词的概率。常见的有n元模型，又叫n元语法。

估计n元模型的概率：

* 最大似然估计（the maximum likelihood estimate），公式：

$$ P(w_i\mid w_{i-1})=\frac{count(w_{i-1},w_i)}{count(w_{i-1})} $$

count表示语料库中括号里字符连续出现的次数。

### 困惑度（perplexity）

困惑度是概率的倒数。困惑度越大，概率越低。困惑度越低，语言模型越好。

### 过拟合（overfitting）

如果训练集有太多特性，在测试集上的表现会不好。因为它过拟合了，进行泛化的能力不行。

### 数据平滑（smoothing）

有时候数据会出现稀疏的情况，有的词出现次数很多，有的词一次也没出现。我们可以给这些词的次数加一，这叫做拉普拉斯平滑（laplace smoothing）。在最大似然估计的基础上给每个类型的样本加1。V表示样本种类数。

$$ P(w_i\mid w_{i-1})=\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V} $$

还有一种古德-图灵平滑方法（Good-Turing Tuning）

当要估计的数据在样本中没有出现时：

$$ P_{GT}=\frac{N_1}{N} $$

$$ N_1 $$表示计算样本中出现一次的数据出现的次数。N表示样本中出现的数据的总次数。

当要估计的数据在样本中有出现时：

$$ c* = \frac{(c+1)N_{c+1}}{N_c} $$

## 噪声信道模型（noisy channel model）

比如我们有一个拼写错误，我们要找到对应的正确的拼写。所通过的方法就是噪声信道。

我们可以产生一系列候选的正确拼写，然后看哪个词在句子中的可能性最大，哪个词就应该是原来的正确拼写。

## 文本分类

比如分出垃圾邮件、确定红楼梦的作者、确定评论是好是坏。

我们可以制定规则来进行分类，或者基于标记的文本适用有监督的机器学习方法进行分类。

有几种机器学习的分类方法，其中一种是朴素贝叶斯方法（naive bayes intuition）。它之所以叫朴素，是因为使用了词袋（bag of words）模型，认为语料库中的词语先后顺序与意义无关。这样可以简化概率的计算问题。

一个关于择偶的实际例子：

$$ P(嫁\mid (帅、有钱、性格好))=\frac{P(帅、有钱、性格好\mid 嫁)P(嫁)}{P(帅、有钱、性格好)}\\ =\frac{P(帅\mid 嫁)P(有钱\mid 嫁)P(性格好\mid 嫁)P(嫁)}{P(帅)P(有钱)P(性格好)} $$

## 自然语言处理结果的评价

主要用到准确率（accuracy）、精确率（precision）和召回率（recall）。

              |  correct  |  not correct
--------------|-----------|-------------
selected      |  tp       |  fp
not selected  |  fn       |  tn

具体的例子可以见我前面的判断意义里讲的。

精确率=tp/(tp+fp)；召回率=tp/(tp+fn)；准确率=(tp+fn+tn)/(tp+fp+fn+tn)


## 生成模型（joint or generative）与条件（conditional）模型

朴素贝叶斯是生成模型，它估计的是联合概率分布。而条件模型算的是条件概率分布，最大熵模型便是其中一种。

## 词-文档关联矩阵（term-document incidence matrices）

这是有关信息检索的内容。举个例子，怎么找到某篇莎士比亚的文章包含了Caesar却不含Calpurnia？我们可以进行全文的搜索，但速度比较慢。这时我们可以建这样一张表，表头的栏目莎士比亚的作品名，纵列是每个单词在作品中是否出现（1表示出现，0表示不出现）。这样以后再搜索就比较容易了。但是如果文本很多的话，这个表会变得很大。因此又有了倒排索引，采用词+文档id的办法。

注意倒排索引（inverted index）的翻译可能不是很好理解，我这里引用![知乎上](https://www.zhihu.com/question/23202010)的解释：

    一个未经处理的数据库中，一般是以文档ID作为索引，以文档内容作为记录。
    而Inverted index 指的是将单词或记录作为索引，将文档ID作为记录，这样便可以方便地通过单词或记录查找到其所在的文档。

# 概率统计

我不太懂这个，只做一些收集整理。

试验：一个婴孩出生，是男是女？

样本空间：一个试验的全部可能出现的结果的集合。

例如一个家有两个孩子，样本空间$$ \Omega $$={男男、男女、女女}

事件：

试验可能结果的集合，样本空间的子集。

比如：

* 事件A：至少一个男孩={男女、男男}
* 事件B：至少一个女孩={男女、女女}

事件运算：

* 积：两个事件同时发生。交集：$$ A \cap B = \{男女\} $$
* 和：两个事件至少有一个发生。并集： $$ A \cup B = \{男男、男女、女女\} $$
* 差：A发生，B不发生。差集： $$ A - B = \{男男\} $$
* 对立：事件A的对立事件。补集：{女女}

联合概率：事件A和B同时出现的概率：$$ P(A \cap B) $$，也可以表示为P(A,B)，P(AB)。

条件概率：$$ P(A\mid B) $$ 事件B发生的条件下，事件A发生的概率。

贝叶斯公式：$$ P(A\mid B)=\frac{P(A,B)}{P(B)} $$ 或者这样表示： $$ P(A\mid B) = \frac{P(B \mid A)P(A)}{P(B)} $$

概率的乘法公式：

$$ P(A,B) = P(B) \times P(A \mid B) = P(A) \times P(B \mid A) $$ ，如果事件独立，那么 $$ P(A,B) = P(A) \times P(B) $$

如果是在某个条件下发生的事件独立，就叫条件独立。


