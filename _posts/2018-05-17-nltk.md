---
date: 2018-05-17 21:24:50+08:00
layout: post
title: NLTK使用笔记
categories: 技术随笔
tags: NLP
---

NLTK是常用的Python自然语言处理库，因为它把算法、模型和语料等都封装好了，我们只需学会怎么使用。因此，这篇文章主要的作用是记录如何使用nltk。

## 切分（Tokenization）

切分可以分为句子切分和词切分，nltk默认用PunktSentenceTokenizer分句，用基于宾州树库分词规范的TreebankWordTokenizer分词。具体用以下命令做到：

```
#句切分
>>> para = "Hello World! Isn't it good to see you? Thanks for buying this book."
>>> from nltk.tokenize import sent_tokenize
>>> sent_tokenize(para)
['Hello World!', "Isn't it good to see you?", 'Thanks for buying this book.']
```

```
#词切分
>>> from nltk.tokenize import word_tokenize 
>>> word_tokenize('Hello World.') 
['Hello', 'World', '.']
```

默认的分词在遇到can't这样的缩写(contraction)时会分成ca和n't，如果不喜欢，可以用`dir(nltk.tokenize)`查看别的分词器并使用。

我们还可以用正则来进行切分。上面提到的方法不支持中文，我们可以用正则来做一个中文分句（中文分词就太难了），方法如下：

```
#中文正则分句
>>> from nltk.tokenize import RegexpTokenizer
>>> tokenizer = RegexpTokenizer(".*?[。！？]")
>>> tokenizer.tokenize("世界真大。我想去看看！你想去吗？不了。")
['世界真大。', '我想去看看！', '你想去吗？', '不了。']
```

待更新……





